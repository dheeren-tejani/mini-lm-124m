{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u3-XUeAV1Cc",
        "outputId": "ae86fb9d-46cf-463a-f9bd-c28a1af6a8ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.49.0\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Package Installation\n",
        "# ============================================================================\n",
        "\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p0_pr7KmSRTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53fe2aff-bf4a-424c-8c0c-e8e467b5a10d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Libraries imported\n",
            "üîß PyTorch version: 2.9.0+cu126\n",
            "üéÆ CUDA available: True\n",
            "üéÆ GPU: Tesla T4\n",
            "üíæ GPU Memory: 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Import Libraries and Mount Drive\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "import threading\n",
        "import queue\n",
        "from datasets import load_dataset\n",
        "import tiktoken\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import bitsandbytes as bnb\n",
        "import shutil\n",
        "import gc\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(f\"‚úÖ Libraries imported\")\n",
        "print(f\"üîß PyTorch version: {torch.__version__}\")\n",
        "print(f\"üéÆ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ce8e3n5JSXKD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b6cbe57-d0ce-4343-d2c6-16c54137369d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TRAINING CONFIGURATION\n",
            "============================================================\n",
            "Model Parameters: ~162M\n",
            "Sequence Length: 1024\n",
            "Batch Size: 6 √ó 21 = 126\n",
            "Learning Rate: 5e-05\n",
            "Max Steps: 25000\n",
            "Checkpoint Every: 100 steps\n",
            "Mixed Precision: True\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Configuration\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Training configuration\"\"\"\n",
        "\n",
        "    # Paths\n",
        "    DRIVE_DATA_DIR = \"/content/drive/MyDrive/llm_training/data\"\n",
        "    DRIVE_CHECKPOINT_DIR = \"/content/drive/MyDrive/llm_training/checkpoints\"\n",
        "    LOCAL_CACHE_DIR = \"/content/training_cache\"\n",
        "\n",
        "    # Model architecture\n",
        "    VOCAB_SIZE = 50257  # GPT-2 tokenizer vocab size\n",
        "    D_MODEL = 768       # Model dimension\n",
        "    N_LAYERS = 12       # Number of transformer layers\n",
        "    N_HEADS = 12         # Number of attention heads\n",
        "    D_FF = 3072         # Feed-forward dimension\n",
        "    MAX_SEQ_LEN = 1024  # Maximum sequence length\n",
        "    DROPOUT = 0.1\n",
        "\n",
        "    # Training hyperparameters\n",
        "    BATCH_SIZE = 6\n",
        "    GRADIENT_ACCUM_STEPS = 21\n",
        "    LEARNING_RATE = 5e-5\n",
        "    WEIGHT_DECAY = 0.1\n",
        "    MAX_GRAD_NORM = 1.0\n",
        "    WARMUP_STEPS = 1750\n",
        "\n",
        "    # Data loading\n",
        "    CHUNK_SIZE_GB = 30      # Load 30GB at a time to Colab temp\n",
        "    PREFETCH_CHUNKS = 1     # Number of chunks to prefetch\n",
        "\n",
        "    # Checkpointing and logging\n",
        "    CHECKPOINT_EVERY = 100  # Steps\n",
        "    LOG_EVERY = 10           # Steps\n",
        "    SAMPLE_EVERY = 100       # Generate samples\n",
        "    SAVE_TOTAL_LIMIT = 3     # Keep only last 3 checkpoints\n",
        "\n",
        "    # Training\n",
        "    MAX_STEPS = 25000       # Total training steps\n",
        "    USE_AMP = True           # Mixed precision training\n",
        "\n",
        "    @classmethod\n",
        "    def estimate_params(cls):\n",
        "        \"\"\"Estimate total parameters\"\"\"\n",
        "        # Embedding\n",
        "        embed_params = cls.VOCAB_SIZE * cls.D_MODEL\n",
        "\n",
        "        # Transformer blocks\n",
        "        # Each block: 4*d_model*d_model (attention) + 2*d_model*d_ff (FFN) + layer norms\n",
        "        per_layer = 4 * cls.D_MODEL**2 + 2 * cls.D_MODEL * cls.D_FF\n",
        "        transformer_params = cls.N_LAYERS * per_layer\n",
        "\n",
        "        # Output head\n",
        "        output_params = cls.D_MODEL * cls.VOCAB_SIZE\n",
        "\n",
        "        total = embed_params + transformer_params + output_params\n",
        "        return total / 1e6  # Return in millions\n",
        "\n",
        "    @classmethod\n",
        "    def print_config(cls):\n",
        "        print(\"=\"*60)\n",
        "        print(\"TRAINING CONFIGURATION\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Model Parameters: ~{cls.estimate_params():.0f}M\")\n",
        "        print(f\"Sequence Length: {cls.MAX_SEQ_LEN}\")\n",
        "        print(f\"Batch Size: {cls.BATCH_SIZE} √ó {cls.GRADIENT_ACCUM_STEPS} = {cls.BATCH_SIZE * cls.GRADIENT_ACCUM_STEPS}\")\n",
        "        print(f\"Learning Rate: {cls.LEARNING_RATE}\")\n",
        "        print(f\"Max Steps: {cls.MAX_STEPS}\")\n",
        "        print(f\"Checkpoint Every: {cls.CHECKPOINT_EVERY} steps\")\n",
        "        print(f\"Mixed Precision: {cls.USE_AMP}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(Config.DRIVE_DATA_DIR, exist_ok=True)\n",
        "os.makedirs(Config.DRIVE_CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(Config.LOCAL_CACHE_DIR, exist_ok=True)\n",
        "\n",
        "Config.print_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CRwAQ3uabK_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b75975-3b72-4d24-c0d3-1f1940bd2412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Smart Dataset loaded!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Complete Smart Dataset\n",
        "# ============================================================================\n",
        "\n",
        "import threading\n",
        "\n",
        "class SmartResumeTokenDataset(Dataset):\n",
        "    '''Dataset with disk cleaner, smart offset, thread safety'''\n",
        "\n",
        "    def __init__(self, token_files, seq_length, local_cache_dir, start_step=0):\n",
        "        self.token_files = sorted(token_files)\n",
        "        self.seq_length = seq_length\n",
        "        self.local_cache_dir = Path(local_cache_dir)\n",
        "        self.local_cache_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        self._file_locks = {}\n",
        "\n",
        "        # Scan dataset\n",
        "        print(\"üìä Scanning dataset...\")\n",
        "        self.file_token_counts = []\n",
        "        self.total_tokens = 0\n",
        "\n",
        "        for f in tqdm(self.token_files, desc=\"Scanning\"):\n",
        "            try:\n",
        "                tokens_in_file = os.path.getsize(f) // 2\n",
        "                self.file_token_counts.append(tokens_in_file)\n",
        "                self.total_tokens += tokens_in_file\n",
        "            except: pass\n",
        "\n",
        "        self.num_sequences = self.total_tokens // seq_length\n",
        "\n",
        "        # Smart offset calculation\n",
        "        tokens_per_step = Config.BATCH_SIZE * Config.MAX_SEQ_LEN * Config.GRADIENT_ACCUM_STEPS\n",
        "        tokens_consumed = start_step * tokens_per_step\n",
        "        self.start_offset = tokens_consumed // seq_length\n",
        "\n",
        "        # Calculate starting file\n",
        "        cumulative = 0\n",
        "        start_file_idx = 0\n",
        "        for i, count in enumerate(self.file_token_counts):\n",
        "            if cumulative + count > tokens_consumed:\n",
        "                start_file_idx = i\n",
        "                break\n",
        "            cumulative += count\n",
        "\n",
        "        print(f\"‚úÖ Dataset ready: {self.total_tokens:,} tokens\")\n",
        "        if start_step > 0:\n",
        "            print(f\"üéØ Smart Resume: Starting from file #{start_file_idx}\")\n",
        "\n",
        "        self.current_chunk = None\n",
        "        self.current_file_idx = -1\n",
        "        self.files_seen = set()\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, self.num_sequences - self.start_offset)\n",
        "\n",
        "    def _get_file_lock(self, file_idx):\n",
        "        if file_idx not in self._file_locks:\n",
        "            self._file_locks[file_idx] = threading.Lock()\n",
        "        return self._file_locks[file_idx]\n",
        "\n",
        "    def _clean_cache(self, keep_file_name=None):\n",
        "        '''Delete old cached files to prevent disk full'''\n",
        "        for f in self.local_cache_dir.glob('*.npy'):\n",
        "            if keep_file_name and f.name == keep_file_name:\n",
        "                continue\n",
        "            try: f.unlink()\n",
        "            except: pass\n",
        "\n",
        "    def _load_file(self, file_idx):\n",
        "        source_file = self.token_files[file_idx]\n",
        "        cache_file = self.local_cache_dir / source_file.name\n",
        "        lock = self._get_file_lock(file_idx)\n",
        "\n",
        "        with lock:\n",
        "            if cache_file.exists():\n",
        "                try: return np.load(cache_file, mmap_mode='r')\n",
        "                except: cache_file.unlink()\n",
        "\n",
        "            # Clean cache before loading new file\n",
        "            self._clean_cache(keep_file_name=source_file.name)\n",
        "\n",
        "            print(f\"üì• Loading {source_file.name}...\")\n",
        "            temp_file = self.local_cache_dir / f\".tmp_{source_file.name}\"\n",
        "            try:\n",
        "                tokens = np.load(source_file)\n",
        "                np.save(temp_file, tokens)\n",
        "                temp_file.rename(cache_file)\n",
        "                return np.load(cache_file, mmap_mode='r')\n",
        "            except Exception as e:\n",
        "                if temp_file.exists(): temp_file.unlink()\n",
        "                raise RuntimeError(f\"Failed to load: {e}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        actual_idx = self.start_offset + idx\n",
        "        if actual_idx >= self.num_sequences:\n",
        "            actual_idx = actual_idx % self.num_sequences\n",
        "\n",
        "        token_pos = actual_idx * self.seq_length\n",
        "\n",
        "        # Find file\n",
        "        cumsum = 0\n",
        "        file_idx = 0\n",
        "        for i, count in enumerate(self.file_token_counts):\n",
        "            if cumsum + count > token_pos:\n",
        "                file_idx = i\n",
        "                break\n",
        "            cumsum += count\n",
        "\n",
        "        if file_idx != self.current_file_idx:\n",
        "            self.current_chunk = self._load_file(file_idx)\n",
        "            self.current_file_idx = file_idx\n",
        "            self.files_seen.add(file_idx)\n",
        "\n",
        "        pos_in_file = token_pos - sum(self.file_token_counts[:file_idx])\n",
        "\n",
        "        if pos_in_file + self.seq_length + 1 > len(self.current_chunk):\n",
        "            seq = self.current_chunk[pos_in_file:]\n",
        "            if len(seq) < self.seq_length + 1:\n",
        "                seq = np.pad(seq, (0, self.seq_length + 1 - len(seq)), constant_values=0)\n",
        "        else:\n",
        "            seq = self.current_chunk[pos_in_file:pos_in_file + self.seq_length + 1]\n",
        "\n",
        "        x = torch.from_numpy(seq[:-1].copy().astype(np.int64))\n",
        "        y = torch.from_numpy(seq[1:].copy().astype(np.int64))\n",
        "        return x, y\n",
        "\n",
        "print(\"‚úÖ Smart Dataset loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5B: MIXED DATALOADER WITH EXPLICIT STATE TRACKING (FIXED)\n",
        "# ============================================================================\n",
        "# This version replaces \"mathematical estimation\" with a physical \"bookmark\" file.\n",
        "# It saves the current file index to 'dataset_state.json' every time a new file loads.\n",
        "# ============================================================================\n",
        "\n",
        "import threading\n",
        "import json\n",
        "import time\n",
        "from typing import List, Dict, Tuple\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tiktoken\n",
        "\n",
        "class DatasetStateManager:\n",
        "    \"\"\"\n",
        "    Manages a simple JSON file that tracks the last used file index for each dataset.\n",
        "    Location: Config.DRIVE_CHECKPOINT_DIR / dataset_state.json\n",
        "    \"\"\"\n",
        "    def __init__(self, checkpoint_dir: Path):\n",
        "        self.state_file = checkpoint_dir / \"dataset_state.json\"\n",
        "        self._lock = threading.Lock()\n",
        "        self.state = self._load_state()\n",
        "\n",
        "    def _load_state(self) -> Dict[str, int]:\n",
        "        if self.state_file.exists():\n",
        "            try:\n",
        "                with open(self.state_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "            except:\n",
        "                return {}\n",
        "        return {}\n",
        "\n",
        "    def update(self, dataset_name: str, file_idx: int):\n",
        "        \"\"\"Update the file index for a dataset and save to disk immediately\"\"\"\n",
        "        with self._lock:\n",
        "            self.state[dataset_name] = file_idx\n",
        "            try:\n",
        "                with open(self.state_file, 'w') as f:\n",
        "                    json.dump(self.state, f)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Failed to save dataset state: {e}\")\n",
        "\n",
        "    def get_start_file(self, dataset_name: str) -> int:\n",
        "        \"\"\"Get the last saved file index, or 0 if new\"\"\"\n",
        "        return self.state.get(dataset_name, 0)\n",
        "\n",
        "class TokenPool:\n",
        "    \"\"\"Manages token files for a dataset with explicit state tracking\"\"\"\n",
        "\n",
        "    def __init__(self, token_files: List[Path], name: str, cache_dir: Path, tokenizer, state_manager: DatasetStateManager):\n",
        "        self.token_files = sorted(token_files)\n",
        "        self.name = name\n",
        "        self.cache_dir = cache_dir / name\n",
        "        self.cache_dir.mkdir(exist_ok=True, parents=True)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.state_manager = state_manager\n",
        "\n",
        "        # Scan files to build local index (needed for intra-file navigation if strictly required,\n",
        "        # but here we rely principally on file_idx from state_manager)\n",
        "        self.file_token_counts = []\n",
        "        self.total_tokens = 0\n",
        "        for f in self.token_files:\n",
        "            try:\n",
        "                # Fast size check (files are uint16, so bytes/2)\n",
        "                count = os.path.getsize(f) // 2\n",
        "                self.file_token_counts.append(count)\n",
        "                self.total_tokens += count\n",
        "            except:\n",
        "                self.file_token_counts.append(0)\n",
        "\n",
        "        # LOAD STATE: Resume explicitly from the saved file index\n",
        "        self.current_file_idx = self.state_manager.get_start_file(name)\n",
        "\n",
        "        # Validation: Ensure index is within bounds\n",
        "        if self.current_file_idx >= len(self.token_files):\n",
        "            self.current_file_idx = 0\n",
        "\n",
        "        self.current_chunk = None\n",
        "        self._file_lock = threading.Lock()\n",
        "\n",
        "        # Pre-load the starting file immediately so we are ready\n",
        "        if self.total_tokens > 0:\n",
        "            print(f\"   ‚Ü™ {name}: Resuming explicitly at File #{self.current_file_idx}\")\n",
        "            self._load_file(self.current_file_idx)\n",
        "\n",
        "    def get_sequence(self, cursor: int, seq_length: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Get sequence.\n",
        "        Note: 'cursor' argument is kept for compatibility with the scheduler,\n",
        "        but we primarily drive data flow by iterating through our current loaded chunk.\n",
        "        \"\"\"\n",
        "        if self.total_tokens == 0:\n",
        "            return np.full(seq_length + 1, self.tokenizer.eot_token, dtype=np.uint16)\n",
        "\n",
        "        # If chunk is not loaded or we ran off the end (shouldn't happen with logic below), reload\n",
        "        if self.current_chunk is None:\n",
        "            self._load_file(self.current_file_idx)\n",
        "\n",
        "        chunk_len = len(self.current_chunk)\n",
        "        max_start = chunk_len - seq_length - 1\n",
        "\n",
        "        if max_start <= 0:\n",
        "            # File too small, just pad\n",
        "            seq = self.current_chunk.copy()\n",
        "            return np.pad(seq, (0, seq_length + 1 - len(seq)), constant_values=self.tokenizer.eot_token)\n",
        "\n",
        "        # Use the cursor to pick a spot in the current file, wrapping around\n",
        "        start_idx = (cursor * 1024) % max_start # Arbitrary stride to utilize the file\n",
        "\n",
        "        seq = self.current_chunk[start_idx : start_idx + seq_length + 1].copy()\n",
        "\n",
        "        if not hasattr(self, '_access_count'): self._access_count = 0\n",
        "        self._access_count += 1\n",
        "\n",
        "        # Approx 1 file worth of context (assuming 10M tokens per file)\n",
        "        # 10M tokens / 1024 seq_len ~= 10,000 samples\n",
        "        if self._access_count > 10000:\n",
        "            self._access_count = 0\n",
        "            next_idx = (self.current_file_idx + 1) % len(self.token_files)\n",
        "            self._load_file(next_idx)\n",
        "\n",
        "        return seq\n",
        "\n",
        "    def _load_file(self, file_idx: int):\n",
        "        \"\"\"Load file and SAVE STATE to Drive\"\"\"\n",
        "        if not self.token_files: return\n",
        "\n",
        "        source_file = self.token_files[file_idx]\n",
        "        cache_file = self.cache_dir / source_file.name\n",
        "\n",
        "        with self._file_lock:\n",
        "            # 1. Update the JSON state file immediately\n",
        "            print(f\"üì• [{self.name}] Loading File #{file_idx} ({source_file.name}) -> Updating state.json\")\n",
        "            self.state_manager.update(self.name, file_idx)\n",
        "\n",
        "            # 2. Load data\n",
        "            if cache_file.exists():\n",
        "                try:\n",
        "                    self.current_chunk = np.load(cache_file, mmap_mode='r')\n",
        "                    self.current_file_idx = file_idx\n",
        "                    return\n",
        "                except:\n",
        "                    cache_file.unlink()\n",
        "\n",
        "            # Clean old cache\n",
        "            for old_file in self.cache_dir.glob('*.npy'):\n",
        "                try: old_file.unlink()\n",
        "                except: pass\n",
        "\n",
        "            try:\n",
        "                tokens = np.load(source_file)\n",
        "                np.save(cache_file, tokens)\n",
        "                self.current_chunk = np.load(cache_file, mmap_mode='r')\n",
        "                self.current_file_idx = file_idx\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(f\"Failed to load {source_file}: {e}\")\n",
        "\n",
        "class HybridMixedDatasetLoader(Dataset):\n",
        "    def __init__(\n",
        "        self, c4_files, cosmopedia_files, alpaca_files, python_files,\n",
        "        seq_length, local_cache_dir, tokenizer, current_training_step, dataset_probs, schedule_length=100000\n",
        "    ):\n",
        "        self.seq_length = seq_length\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Initialize State Manager (The \"Bookmark\" System)\n",
        "        self.state_manager = DatasetStateManager(Path(Config.DRIVE_CHECKPOINT_DIR))\n",
        "\n",
        "        # Force Python/Alpaca to always restart at 0 (they are small & loopable)\n",
        "        # We only want to track C4 and Cosmopedia persistence\n",
        "        # (Optional: you can remove this if you want to track them too)\n",
        "        self.state_manager.update(\"alpaca\", 0)\n",
        "        self.state_manager.update(\"python\", 0)\n",
        "\n",
        "        # Standard Setup\n",
        "        cache_path = Path(local_cache_dir)\n",
        "        cache_path.mkdir(exist_ok=True, parents=True)\n",
        "        self.datasets = {}\n",
        "\n",
        "        if c4_files: self.datasets['c4'] = TokenPool(c4_files, 'c4', cache_path, tokenizer, self.state_manager)\n",
        "        if cosmopedia_files: self.datasets['cosmopedia'] = TokenPool(cosmopedia_files, 'cosmopedia', cache_path, tokenizer, self.state_manager)\n",
        "        if alpaca_files: self.datasets['alpaca'] = TokenPool(alpaca_files, 'alpaca', cache_path, tokenizer, self.state_manager)\n",
        "        if python_files: self.datasets['python'] = TokenPool(python_files, 'python', cache_path, tokenizer, self.state_manager)\n",
        "\n",
        "        # Renormalize Probs\n",
        "        self.datasets = {k: v for k, v in self.datasets.items() if v.total_tokens > 0}\n",
        "        available = set(self.datasets.keys())\n",
        "        dataset_probs = {k: v for k, v in dataset_probs.items() if k in available}\n",
        "        total = sum(dataset_probs.values())\n",
        "        self.dataset_probs = {k: v/total for k,v in dataset_probs.items()}\n",
        "\n",
        "        # Schedule\n",
        "        self.schedule_length = schedule_length\n",
        "        self.mixing_schedule = []\n",
        "        rng = np.random.RandomState(42)\n",
        "        names = list(self.dataset_probs.keys())\n",
        "        probs = [self.dataset_probs[n] for n in names]\n",
        "        for _ in range(schedule_length):\n",
        "            self.mixing_schedule.append(rng.choice(names, p=probs))\n",
        "\n",
        "        self.total_sequences = 10_000_000_000\n",
        "        print(f\"‚úÖ State-Aware Hybrid Loader Initialized\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # We just cycle through the schedule\n",
        "        dataset_name = self.mixing_schedule[idx % self.schedule_length]\n",
        "\n",
        "        # Pass the idx as a cursor so we move through the file\n",
        "        # The TokenPool handles the actual \"File Switching\" logic\n",
        "        x_tokens = self.datasets[dataset_name].get_sequence(idx, self.seq_length)\n",
        "\n",
        "        x = torch.from_numpy(x_tokens[:-1].copy().astype(np.int64))\n",
        "        y = torch.from_numpy(x_tokens[1:].copy().astype(np.int64))\n",
        "        return x, y\n",
        "\n",
        "def setup_mixed_dataloader(current_step: int = 0):\n",
        "    print(\"=\"*70)\n",
        "    print(\"SETTING UP MIXED DATALOADER (STATE-AWARE VERSION)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    base_dir = Path(Config.DRIVE_DATA_DIR)\n",
        "\n",
        "    # Files\n",
        "    c4 = sorted(base_dir.glob(\"tokens_*.npy\"))\n",
        "    cosmo = sorted((base_dir / \"cosmopedia\").glob(\"cosmopedia_tokens_*.npy\"))\n",
        "    alpaca = sorted((base_dir / \"alpaca\").glob(\"alpaca_tokens_*.npy\"))\n",
        "    python = sorted((base_dir / \"python\").glob(\"python_tokens_*.npy\"))\n",
        "\n",
        "    # Probs\n",
        "    probs = {'cosmopedia': 0.50, #'c4': 0.00,\n",
        "             'alpaca': 0.30, 'python': 0.20}\n",
        "    if not python:\n",
        "        probs = {'cosmopedia': 0.50, #'c4': 0.34,\n",
        "                 'alpaca': 0.50}\n",
        "\n",
        "    # Initialize\n",
        "    # Note: We don't pass 'current_step' anymore because the StateManager\n",
        "    # reads the actual file index from disk!\n",
        "    mixed_dataset = HybridMixedDatasetLoader(\n",
        "        c4_files=c4, cosmopedia_files=cosmo, alpaca_files=alpaca, python_files=python,\n",
        "        seq_length=Config.MAX_SEQ_LEN, local_cache_dir=Config.LOCAL_CACHE_DIR,\n",
        "        tokenizer=tokenizer, current_training_step=current_step, dataset_probs=probs\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(mixed_dataset, batch_size=Config.BATCH_SIZE, num_workers=1, pin_memory=True)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "jMU3qeDU6sCP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TTYNSz0NoClh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3acbdbe-1e41-4a45-e708-17e1471e4b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model architecture defined!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Model Architecture (GPT-2 style)\n",
        "# ============================================================================\n",
        "\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # QKV projection\n",
        "        qkv = self.qkv_proj(x)\n",
        "        qkv = qkv.reshape(batch_size, seq_len, 3, self.n_heads, self.d_k)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch, heads, seq, d_k)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
        "\n",
        "        # Apply causal mask\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().reshape(batch_size, seq_len, d_model)\n",
        "\n",
        "        return self.out_proj(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Pre-norm architecture (like GPT-2)\n",
        "        x = x + self.dropout(self.attn(self.ln1(x), mask))\n",
        "        x = x + self.dropout(self.ff(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Embeddings\n",
        "        self.token_embed = nn.Embedding(config.VOCAB_SIZE, config.D_MODEL)\n",
        "        self.pos_embed = nn.Embedding(config.MAX_SEQ_LEN, config.D_MODEL)\n",
        "        self.dropout = nn.Dropout(config.DROPOUT)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(config.D_MODEL, config.N_HEADS, config.D_FF, config.DROPOUT)\n",
        "            for _ in range(config.N_LAYERS)\n",
        "        ])\n",
        "\n",
        "        # Output\n",
        "        self.ln_final = nn.LayerNorm(config.D_MODEL)\n",
        "        self.head = nn.Linear(config.D_MODEL, config.VOCAB_SIZE, bias=False)\n",
        "\n",
        "        # Tie weights\n",
        "        self.token_embed.weight = self.head.weight\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        print(f\"‚úÖ Model initialized with {self.count_parameters()/1e6:.2f}M parameters\")\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "\n",
        "        # Embeddings\n",
        "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n",
        "        x = self.token_embed(x) + self.pos_embed(positions)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Causal mask\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # Transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        # Output\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "print(\"‚úÖ Model architecture defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_Jyp59cooDV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "151d1f1c-1130-4efa-b22d-889d29e6cfd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training utilities defined!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Training Utilities\n",
        "# ============================================================================\n",
        "\n",
        "class TrainingLogger:\n",
        "    \"\"\"Comprehensive logging system\"\"\"\n",
        "\n",
        "    def __init__(self, log_dir):\n",
        "        self.log_dir = Path(log_dir)\n",
        "        self.log_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        # Log file\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.log_file = self.log_dir / f\"training_{timestamp}.log\"\n",
        "\n",
        "        # Metrics history\n",
        "        self.metrics = {\n",
        "            'step': [],\n",
        "            'loss': [],\n",
        "            'lr': [],\n",
        "            'tokens_per_sec': [],\n",
        "            'gpu_mem_gb': []\n",
        "        }\n",
        "\n",
        "    def log(self, message, print_console=True):\n",
        "        \"\"\"Log message to file and optionally console\"\"\"\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        log_message = f\"[{timestamp}] {message}\"\n",
        "\n",
        "        with open(self.log_file, 'a') as f:\n",
        "            f.write(log_message + '\\n')\n",
        "\n",
        "        if print_console:\n",
        "            print(log_message)\n",
        "\n",
        "    def log_metrics(self, step, loss, lr, tokens_per_sec, gpu_mem_gb):\n",
        "        \"\"\"Log training metrics\"\"\"\n",
        "        self.metrics['step'].append(step)\n",
        "        self.metrics['loss'].append(loss)\n",
        "        self.metrics['lr'].append(lr)\n",
        "        self.metrics['tokens_per_sec'].append(tokens_per_sec)\n",
        "        self.metrics['gpu_mem_gb'].append(gpu_mem_gb)\n",
        "\n",
        "        # Save metrics\n",
        "        metrics_file = self.log_dir / \"metrics.json\"\n",
        "        with open(metrics_file, 'w') as f:\n",
        "            json.dump(self.metrics, f, indent=2)\n",
        "\n",
        "    def print_progress(self, step, loss, lr, tokens_per_sec, gpu_mem_gb, elapsed_time):\n",
        "        \"\"\"Print formatted progress\"\"\"\n",
        "        message = (\n",
        "            f\"Step {step:6d} | \"\n",
        "            f\"Loss: {loss:.4f} | \"\n",
        "            f\"LR: {lr:.2e} | \"\n",
        "            f\"Tokens/s: {tokens_per_sec:,.0f} | \"\n",
        "            f\"GPU: {gpu_mem_gb:.1f}GB | \"\n",
        "            f\"Time: {elapsed_time:.0f}s\"\n",
        "        )\n",
        "        self.log(message)\n",
        "\n",
        "def get_lr(step, warmup_steps, max_lr, max_steps):\n",
        "    \"\"\"Learning rate schedule with warmup and cosine decay\"\"\"\n",
        "    if step < warmup_steps:\n",
        "        # Linear warmup\n",
        "        return max_lr * step / warmup_steps\n",
        "    elif step < max_steps:\n",
        "        # Cosine decay\n",
        "        progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
        "        return max_lr * 0.5 * (1 + np.cos(np.pi * progress))\n",
        "    else:\n",
        "        return max_lr * 0.1\n",
        "\n",
        "def save_checkpoint(model, optimizer, scaler, step, loss, checkpoint_dir, logger):\n",
        "    \"\"\"Save training checkpoint\"\"\"\n",
        "    checkpoint_dir = Path(checkpoint_dir)\n",
        "    checkpoint_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    checkpoint_path = checkpoint_dir / f\"checkpoint_step_{step}.pt\"\n",
        "\n",
        "    checkpoint = {\n",
        "        'step': step,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scaler_state_dict': scaler.state_dict() if scaler else None,\n",
        "        'loss': loss,\n",
        "        'config': {\n",
        "            'vocab_size': Config.VOCAB_SIZE,\n",
        "            'd_model': Config.D_MODEL,\n",
        "            'n_layers': Config.N_LAYERS,\n",
        "            'n_heads': Config.N_HEADS,\n",
        "            'd_ff': Config.D_FF,\n",
        "            'max_seq_len': Config.MAX_SEQ_LEN,\n",
        "            'dropout': Config.DROPOUT,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, checkpoint_path)\n",
        "    logger.log(f\"üíæ Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    # Keep only last N checkpoints\n",
        "    checkpoints = sorted(checkpoint_dir.glob(\"checkpoint_step_*.pt\"), key=lambda p: int(p.stem.split('_')[-1]))\n",
        "    if len(checkpoints) > Config.SAVE_TOTAL_LIMIT:\n",
        "        for old_checkpoint in checkpoints[:-Config.SAVE_TOTAL_LIMIT]:\n",
        "            old_checkpoint.unlink()\n",
        "            logger.log(f\"üóëÔ∏è  Removed old checkpoint: {old_checkpoint.name}\")\n",
        "\n",
        "    return checkpoint_path\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, scaler):\n",
        "    \"\"\"Load checkpoint\"\"\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cuda', weights_only=False)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scaler and checkpoint['scaler_state_dict']:\n",
        "        scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "\n",
        "    return checkpoint['step'], checkpoint['loss']\n",
        "\n",
        "def generate_sample(model, tokenizer, prompt=\"The future of AI is\", max_length=100, temperature=0.8):\n",
        "    \"\"\"Generate text sample\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long, device='cuda').unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            if tokens.size(1) >= Config.MAX_SEQ_LEN:\n",
        "                tokens = tokens[:, -Config.MAX_SEQ_LEN:]\n",
        "\n",
        "            logits = model(tokens)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            tokens = torch.cat([tokens, next_token], dim=1)\n",
        "\n",
        "            # Stop at end of text token\n",
        "            if next_token.item() == tokenizer.eot_token:\n",
        "                break\n",
        "\n",
        "    generated = tokenizer.decode(tokens[0].cpu().numpy())\n",
        "    model.train()\n",
        "\n",
        "    return generated\n",
        "\n",
        "print(\"‚úÖ Training utilities defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "u6RzLkFMoFWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e886d0f3-a9d3-42dd-8f00-320ea11bf05f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fixed training function loaded (handles torch.compile checkpoints)!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Cell 8 - Complete Training Function (Handles torch.compile checkpoints)\n",
        "# ============================================================================\n",
        "\n",
        "def train_complete():\n",
        "    '''Complete training with smart resume, disk cleaning, error handling'''\n",
        "\n",
        "    logger = TrainingLogger(Config.DRIVE_CHECKPOINT_DIR)\n",
        "    logger.log(\"=\"*60)\n",
        "    logger.log(\"STARTING TRAINING\")\n",
        "    logger.log(\"=\"*60)\n",
        "\n",
        "    # Find token files\n",
        "    token_files = list(Path(Config.DRIVE_DATA_DIR).glob(\"tokens_*.npy\"))\n",
        "    if not token_files:\n",
        "        logger.log(\"‚ùå No token files!\")\n",
        "        return\n",
        "\n",
        "    # Check for checkpoint\n",
        "    checkpoints = sorted(Path(Config.DRIVE_CHECKPOINT_DIR).glob(\"checkpoint_step_*.pt\"))\n",
        "    start_step = 0\n",
        "\n",
        "    # Create model\n",
        "    logger.log(\"üèóÔ∏è  Building model...\")\n",
        "    model = GPTModel(Config).cuda()\n",
        "\n",
        "    optimizer = bnb.optim.AdamW8bit(\n",
        "        model.parameters(),\n",
        "        lr=Config.LEARNING_RATE,\n",
        "        weight_decay=Config.WEIGHT_DECAY,\n",
        "        betas=(0.9, 0.95)\n",
        "    )\n",
        "\n",
        "    scaler = GradScaler() if Config.USE_AMP else None\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    if checkpoints:\n",
        "        latest = checkpoints[-1]\n",
        "        logger.log(f\"üì• Loading: {latest.name}\")\n",
        "        checkpoint = torch.load(latest, map_location='cuda', weights_only=False)\n",
        "\n",
        "        # ============================================================\n",
        "        # FIX: Handle torch.compile() checkpoints\n",
        "        # ============================================================\n",
        "        state_dict = checkpoint['model_state_dict']\n",
        "\n",
        "        # Check if checkpoint was saved from compiled model\n",
        "        if any(key.startswith('_orig_mod.') for key in state_dict.keys()):\n",
        "            logger.log(\"‚ö†Ô∏è  Checkpoint from compiled model, removing _orig_mod. prefix...\")\n",
        "            # Remove _orig_mod. prefix from all keys\n",
        "            new_state_dict = {}\n",
        "            for key, value in state_dict.items():\n",
        "                new_key = key.replace('_orig_mod.', '')\n",
        "                new_state_dict[new_key] = value\n",
        "            state_dict = new_state_dict\n",
        "        # ============================================================\n",
        "\n",
        "        model.load_state_dict(state_dict)\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        if scaler and checkpoint.get('scaler_state_dict'):\n",
        "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
        "        start_step = checkpoint['step']\n",
        "        logger.log(f\"‚úÖ Resumed from step {start_step}\")\n",
        "\n",
        "    # Create smart dataset\n",
        "    dataset = SmartResumeTokenDataset(\n",
        "        token_files, Config.MAX_SEQ_LEN,\n",
        "        Config.LOCAL_CACHE_DIR, start_step=start_step\n",
        "    )\n",
        "\n",
        "    dataloader = mixed_dataloader\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Training loop\n",
        "    logger.log(\"üöÄ Starting training...\")\n",
        "    model.train()\n",
        "\n",
        "    step = start_step\n",
        "    total_tokens = 0\n",
        "    start_time = time.time()\n",
        "    step_time = time.time()\n",
        "    accum_counter = 0\n",
        "\n",
        "    try:\n",
        "        for batch_idx, (x, y) in enumerate(dataloader):\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "\n",
        "            with autocast(enabled=Config.USE_AMP):\n",
        "                logits = model(x)\n",
        "                loss = nn.functional.cross_entropy(\n",
        "                    logits.view(-1, Config.VOCAB_SIZE), y.view(-1)\n",
        "                )\n",
        "                loss = loss / Config.GRADIENT_ACCUM_STEPS\n",
        "\n",
        "            if Config.USE_AMP: scaler.scale(loss).backward()\n",
        "            else: loss.backward()\n",
        "\n",
        "            accum_counter += 1\n",
        "\n",
        "            if accum_counter % Config.GRADIENT_ACCUM_STEPS == 0:\n",
        "                if Config.USE_AMP:\n",
        "                    scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), Config.MAX_GRAD_NORM)\n",
        "\n",
        "                if Config.USE_AMP:\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    optimizer.step()\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                lr = get_lr(step, Config.WARMUP_STEPS, Config.LEARNING_RATE, Config.MAX_STEPS)\n",
        "                for pg in optimizer.param_groups:\n",
        "                    pg['lr'] = lr\n",
        "\n",
        "                step += 1\n",
        "                total_tokens += Config.BATCH_SIZE * Config.MAX_SEQ_LEN * Config.GRADIENT_ACCUM_STEPS\n",
        "\n",
        "                elapsed = time.time() - step_time\n",
        "                tokens_per_sec = (Config.BATCH_SIZE * Config.MAX_SEQ_LEN * Config.GRADIENT_ACCUM_STEPS) / elapsed\n",
        "                gpu_mem = torch.cuda.max_memory_allocated() / 1e9\n",
        "\n",
        "                if step % Config.LOG_EVERY == 0:\n",
        "                    logger.print_progress(step, loss.item()*Config.GRADIENT_ACCUM_STEPS,\n",
        "                                        lr, tokens_per_sec, gpu_mem, time.time()-start_time)\n",
        "                    # logger.log(f\"   üìä File #{dataset.current_file_idx} | Seen: {len(dataset.files_seen)}/{len(token_files)}\")\n",
        "\n",
        "                if step % Config.SAMPLE_EVERY == 0:\n",
        "                    logger.log(\"\\n\" + \"=\"*60)\n",
        "                    logger.log(\"üìù SAMPLES\")\n",
        "                    for prompt in [\"The future of AI is\", \"Once upon a time,\", \"Physics is\", \"A list is different from a tuple because\", \"A for\", \"Define gravity in one sentence.\"\n",
        "]:\n",
        "                        sample = generate_sample(model, tokenizer, prompt, 50)\n",
        "                        logger.log(f\"{prompt} ‚Üí {sample}\")\n",
        "                    logger.log(\"=\"*60 + \"\\n\")\n",
        "\n",
        "                if step % Config.CHECKPOINT_EVERY == 0:\n",
        "                    save_checkpoint(model, optimizer, scaler, step,\n",
        "                                  loss.item()*Config.GRADIENT_ACCUM_STEPS,\n",
        "                                  Config.DRIVE_CHECKPOINT_DIR, logger)\n",
        "\n",
        "                step_time = time.time()\n",
        "\n",
        "                if step >= Config.MAX_STEPS:\n",
        "                    break\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logger.log(\"\\n‚ö†Ô∏è  Interrupted! Saving...\")\n",
        "        save_checkpoint(model, optimizer, scaler, step,\n",
        "                       loss.item()*Config.GRADIENT_ACCUM_STEPS,\n",
        "                       Config.DRIVE_CHECKPOINT_DIR, logger)\n",
        "        return model\n",
        "\n",
        "    logger.log(\"\\n‚úÖ TRAINING COMPLETE!\")\n",
        "    save_checkpoint(model, optimizer, scaler, step,\n",
        "                   loss.item()*Config.GRADIENT_ACCUM_STEPS,\n",
        "                   Config.DRIVE_CHECKPOINT_DIR, logger)\n",
        "    return model\n",
        "\n",
        "print(\"‚úÖ Fixed training function loaded (handles torch.compile checkpoints)!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jt--d8TLzXuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "802fa36a-750e-431c-8e2b-f27e60330d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "SETTING UP MIXED DATALOADER (STATE-AWARE VERSION)\n",
            "======================================================================\n",
            "   ‚Ü™ c4: Resuming explicitly at File #0\n",
            "üì• [c4] Loading File #0 (tokens_0000.npy) -> Updating state.json\n",
            "   ‚Ü™ cosmopedia: Resuming explicitly at File #6\n",
            "üì• [cosmopedia] Loading File #6 (cosmopedia_tokens_0006.npy) -> Updating state.json\n",
            "   ‚Ü™ alpaca: Resuming explicitly at File #0\n",
            "üì• [alpaca] Loading File #0 (alpaca_tokens_0000.npy) -> Updating state.json\n",
            "   ‚Ü™ python: Resuming explicitly at File #0\n",
            "üì• [python] Loading File #0 (python_tokens_0000.npy) -> Updating state.json\n",
            "‚úÖ State-Aware Hybrid Loader Initialized\n",
            "[2026-01-04 08:23:46] ============================================================\n",
            "[2026-01-04 08:23:46] STARTING TRAINING\n",
            "[2026-01-04 08:23:46] ============================================================\n",
            "[2026-01-04 08:23:46] üèóÔ∏è  Building model...\n",
            "\n",
            "‚ö†Ô∏è  Training interrupted by user\n",
            "üíæ Latest checkpoint saved to Drive\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: START TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "üöÄ START TRAINING\n",
        "\n",
        "This cell will:\n",
        "1. Load tokenized data from Google Drive\n",
        "2. Train the model with automatic checkpointing\n",
        "3. Generate samples periodically\n",
        "4. Save progress to Drive\n",
        "\n",
        "‚ö†Ô∏è  This will take DAYS to complete!\n",
        "‚ö†Ô∏è  Keep Colab Pro active or it will disconnect\n",
        "\n",
        "Press Ctrl+C to stop training (checkpoint will be saved)\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    mixed_dataloader = setup_mixed_dataloader(current_step=0)\n",
        "    trained_model = train_complete()\n",
        "    print(\"\\nüéâ Training completed successfully!\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è  Training interrupted by user\")\n",
        "    print(\"üíæ Latest checkpoint saved to Drive\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training failed with error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Check your folder to confirm the exact name.\n",
        "# It is usually 'loader_state.json' or 'model/loader_state.json'\n",
        "STATE_FILE = f\"{Config.DRIVE_CHECKPOINT_DIR}/dataset_state.json\"\n",
        "\n",
        "def remove_c4_from_state():\n",
        "    if not os.path.exists(STATE_FILE):\n",
        "        # Try looking in the root if not found in model/\n",
        "        if os.path.exists(\"loader_state.json\"):\n",
        "            state_path = \"loader_state.json\"\n",
        "        else:\n",
        "            print(f\"‚ùå Could not find state file at {STATE_FILE} or ./loader_state.json\")\n",
        "            return\n",
        "    else:\n",
        "        state_path = STATE_FILE\n",
        "\n",
        "    print(f\"üìÇ Found state file: {state_path}\")\n",
        "\n",
        "    # 1. Create a Backup (Safety First)\n",
        "    backup_path = state_path + \".bak\"\n",
        "    shutil.copy(state_path, backup_path)\n",
        "    print(f\"‚úÖ Backup created at: {backup_path}\")\n",
        "\n",
        "    # 2. Load JSON\n",
        "    with open(state_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # 3. Check and Remove C4\n",
        "    # The structure usually looks like { \"c4\": {...}, \"cosmopedia\": {...} }\n",
        "    # Or sometimes { \"dataset_states\": { \"c4\": ... } }\n",
        "\n",
        "    modified = False\n",
        "\n",
        "    # Direct Key Check\n",
        "    if \"c4\" in data:\n",
        "        print(f\"üóëÔ∏è  Found 'c4' entry. Removing...\")\n",
        "        del data[\"c4\"]\n",
        "        modified = True\n",
        "\n",
        "    # Nested Key Check (just in case your script nests it)\n",
        "    elif \"dataset_states\" in data and \"c4\" in data[\"dataset_states\"]:\n",
        "        print(f\"üóëÔ∏è  Found nested 'c4' entry. Removing...\")\n",
        "        del data[\"dataset_states\"][\"c4\"]\n",
        "        modified = True\n",
        "\n",
        "    if modified:\n",
        "        # 4. Save Changes\n",
        "        with open(state_path, 'w') as f:\n",
        "            json.dump(data, f, indent=4)\n",
        "        print(f\"üíæ Saved updated state file (C4 removed).\")\n",
        "        print(\"üöÄ You can now restart your training script.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  'c4' key was not found in the file. It might already be clean.\")\n",
        "        print(f\"Current Keys: {list(data.keys())}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    remove_c4_from_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-ckyAHtFbk6",
        "outputId": "bbc64f98-bc3e-4167-9fb8-b04180314f85"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Found state file: /content/drive/MyDrive/llm_training/checkpoints/dataset_state.json\n",
            "‚úÖ Backup created at: /content/drive/MyDrive/llm_training/checkpoints/dataset_state.json.bak\n",
            "üóëÔ∏è  Found 'c4' entry. Removing...\n",
            "üíæ Saved updated state file (C4 removed).\n",
            "üöÄ You can now restart your training script.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVgnnBWSofHS"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Interactive Text Generation\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "üí¨ INTERACTIVE TEXT GENERATION\n",
        "\n",
        "Chat with your model!\n",
        "\"\"\"\n",
        "\n",
        "def interactive_generation(checkpoint_path):\n",
        "    \"\"\"Interactive text generation loop\"\"\"\n",
        "\n",
        "    print(\"üì• Loading model...\")\n",
        "    model = GPTModel(Config).cuda()\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    print(f\"‚úÖ Model loaded from step {checkpoint['step']}\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INTERACTIVE GENERATION\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Enter your prompts (type 'quit' to exit)\")\n",
        "    print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    while True:\n",
        "        prompt = input(\"You: \")\n",
        "\n",
        "        if prompt.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not prompt.strip():\n",
        "            continue\n",
        "\n",
        "        print(\"\\nü§ñ Model: \", end=\"\")\n",
        "\n",
        "        # Generate with streaming\n",
        "        tokens = tokenizer.encode(prompt)\n",
        "        tokens = torch.tensor(tokens, dtype=torch.long, device='cuda').unsqueeze(0)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i in range(150):\n",
        "                if tokens.size(1) >= Config.MAX_SEQ_LEN:\n",
        "                    tokens = tokens[:, -Config.MAX_SEQ_LEN:]\n",
        "\n",
        "                logits = model(tokens)\n",
        "                logits = logits[:, -1, :] / 0.8\n",
        "\n",
        "                probs = torch.softmax(logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                tokens = torch.cat([tokens, next_token], dim=1)\n",
        "\n",
        "                # Decode and print\n",
        "                decoded = tokenizer.decode([next_token.item()])\n",
        "                print(decoded, end=\"\", flush=True)\n",
        "\n",
        "                if next_token.item() == tokenizer.eot_token:\n",
        "                    break\n",
        "\n",
        "        model.train()\n",
        "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
        "\n",
        "# Run interactive mode\n",
        "checkpoints = sorted(Path(Config.DRIVE_CHECKPOINT_DIR).glob(\"checkpoint_step_*.pt\"))\n",
        "if checkpoints:\n",
        "    latest = checkpoints[-1]\n",
        "    run = input(f\"Start interactive generation with {latest.name}? (y/n): \")\n",
        "    if run.lower() == 'y':\n",
        "        interactive_generation(latest)\n",
        "else:\n",
        "    print(\"‚ùå No checkpoints found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7kzDLNaq8zL"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 11: Training Statistics and Visualization\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "üìä VISUALIZE TRAINING PROGRESS\n",
        "\"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "def plot_training_metrics():\n",
        "    \"\"\"Plot training metrics from logs\"\"\"\n",
        "\n",
        "    metrics_file = Path(Config.DRIVE_CHECKPOINT_DIR) / \"metrics.json\"\n",
        "\n",
        "    if not metrics_file.exists():\n",
        "        print(\"‚ùå No metrics file found!\")\n",
        "        return\n",
        "\n",
        "    with open(metrics_file, 'r') as f:\n",
        "        metrics = json.load(f)\n",
        "\n",
        "    if not metrics['step']:\n",
        "        print(\"‚ùå No metrics recorded yet!\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Loss curve\n",
        "    axes[0, 0].plot(metrics['step'], metrics['loss'])\n",
        "    axes[0, 0].set_xlabel('Step')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Training Loss')\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    # Learning rate\n",
        "    axes[0, 1].plot(metrics['step'], metrics['lr'])\n",
        "    axes[0, 1].set_xlabel('Step')\n",
        "    axes[0, 1].set_ylabel('Learning Rate')\n",
        "    axes[0, 1].set_title('Learning Rate Schedule')\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    # Tokens per second\n",
        "    axes[1, 0].plot(metrics['step'], metrics['tokens_per_sec'])\n",
        "    axes[1, 0].set_xlabel('Step')\n",
        "    axes[1, 0].set_ylabel('Tokens/sec')\n",
        "    axes[1, 0].set_title('Training Throughput')\n",
        "    axes[1, 0].grid(True)\n",
        "\n",
        "    # GPU memory\n",
        "    axes[1, 1].plot(metrics['step'], metrics['gpu_mem_gb'])\n",
        "    axes[1, 1].set_xlabel('Step')\n",
        "    axes[1, 1].set_ylabel('GPU Memory (GB)')\n",
        "    axes[1, 1].set_title('GPU Memory Usage')\n",
        "    axes[1, 1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(Path(Config.DRIVE_CHECKPOINT_DIR) / 'training_curves.png', dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"‚úÖ Training curves plotted!\")\n",
        "    print(f\"üìä Total steps: {metrics['step'][-1]}\")\n",
        "    print(f\"üìâ Final loss: {metrics['loss'][-1]:.4f}\")\n",
        "    print(f\"‚ö° Avg throughput: {sum(metrics['tokens_per_sec'])/len(metrics['tokens_per_sec']):,.0f} tokens/sec\")\n",
        "\n",
        "# Plot metrics\n",
        "plot_training_metrics()\n",
        "\n",
        "print(\"\\n‚úÖ ALL CELLS COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"üìö USAGE GUIDE\")\n",
        "print(\"=\"*60)\n",
        "print(\"1. Run Cell 4 to download and tokenize C4 (one-time, 2-6 hours)\")\n",
        "print(\"2. Run Cell 9 to start training (will take days)\")\n",
        "print(\"3. Run Cell 10 to generate text from checkpoints\")\n",
        "print(\"4. Run Cell 11 for interactive generation\")\n",
        "print(\"5. Run Cell 12 to visualize training progress\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DOWNLOAD CELL 1: Data Preprocessing - Tokenize C4 and Save (WITH CHECKPOINTING)\n",
        "# ============================================================================\n",
        "\n",
        "class C4Preprocessor:\n",
        "    \"\"\"Download C4, tokenize, and save to Drive with TRUE resume support\"\"\"\n",
        "\n",
        "    def __init__(self, output_dir, target_size_gb=100):\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True, parents=True)\n",
        "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "        self.target_size_gb = target_size_gb\n",
        "\n",
        "        # Local temp directory for fast saving\n",
        "        self.local_temp_dir = Path(\"/content/c4_temp_tokens\")\n",
        "        self.local_temp_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Progress tracking file\n",
        "        self.progress_file = self.output_dir / \"preprocessing_progress.json\"\n",
        "\n",
        "        # Partial chunk file for true resume\n",
        "        self.partial_chunk_file = self.local_temp_dir / \"partial_chunk.npy\"\n",
        "\n",
        "    def save_progress(self, current_bytes, file_count, current_tokens):\n",
        "        \"\"\"Save preprocessing progress including partial tokens\"\"\"\n",
        "        progress = {\n",
        "            'current_bytes': current_bytes,\n",
        "            'file_count': file_count,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Save progress metadata\n",
        "        with open(self.progress_file, 'w') as f:\n",
        "            json.dump(progress, f)\n",
        "\n",
        "        # Save partial tokens if any\n",
        "        if current_tokens:\n",
        "            token_array = np.array(current_tokens, dtype=np.uint16)\n",
        "            np.save(self.partial_chunk_file, token_array)\n",
        "\n",
        "    def load_progress(self):\n",
        "        \"\"\"Load preprocessing progress\"\"\"\n",
        "        if self.progress_file.exists():\n",
        "            with open(self.progress_file, 'r') as f:\n",
        "                progress = json.load(f)\n",
        "\n",
        "            # Load partial tokens if they exist\n",
        "            if self.partial_chunk_file.exists():\n",
        "                partial_tokens = np.load(self.partial_chunk_file).tolist()\n",
        "                progress['partial_tokens'] = partial_tokens\n",
        "            else:\n",
        "                progress['partial_tokens'] = []\n",
        "\n",
        "            return progress\n",
        "        return None\n",
        "\n",
        "    def preprocess_and_save(self):\n",
        "        \"\"\"Download C4, tokenize, and save as binary files with TRUE resume support\"\"\"\n",
        "\n",
        "        print(f\"üöÄ Starting C4 preprocessing (target: {self.target_size_gb}GB raw text)\")\n",
        "\n",
        "        # Check for existing progress\n",
        "        existing_progress = self.load_progress()\n",
        "        existing_files = sorted(self.output_dir.glob(\"tokens_*.npy\"))\n",
        "\n",
        "        if existing_progress and existing_files:\n",
        "            current_bytes = existing_progress['current_bytes']\n",
        "            file_count = existing_progress['file_count']\n",
        "            current_tokens = existing_progress.get('partial_tokens', [])\n",
        "\n",
        "            print(f\"‚úÖ Found {len(existing_files)} existing token files\")\n",
        "            print(f\"üìä Previous progress: {current_bytes/(1024**3):.2f} GB / {self.target_size_gb} GB\")\n",
        "            print(f\"üì¶ {file_count} complete files, {len(current_tokens):,} tokens in partial chunk\")\n",
        "\n",
        "            if current_bytes >= self.target_size_gb * (1024**3):\n",
        "                print(\"üéâ Target already reached!\")\n",
        "                return existing_files\n",
        "\n",
        "            response = input(\"\\nContinue downloading? (y/n): \")\n",
        "            if response.lower() != 'y':\n",
        "                print(\"‚è≠Ô∏è  Skipping preprocessing\")\n",
        "                return existing_files\n",
        "\n",
        "            print(f\"‚ñ∂Ô∏è  Continuing from {current_bytes/(1024**3):.2f} GB\")\n",
        "        else:\n",
        "            current_bytes = 0\n",
        "            file_count = 0\n",
        "            current_tokens = []\n",
        "            print(\"üÜï Starting new preprocessing...\")\n",
        "\n",
        "        # Load C4 streaming - NO SKIPPING, just continue from where we left off\n",
        "        print(\"üì• Loading C4 dataset (streaming)...\")\n",
        "        dataset = load_dataset(\n",
        "            \"allenai/c4\",\n",
        "            \"en\",\n",
        "            split=\"train\",\n",
        "            streaming=True,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        target_bytes = self.target_size_gb * (1024**3)\n",
        "        tokens_per_file = 10_000_000  # ~10M tokens per file (~20MB)\n",
        "\n",
        "        # For periodic saves\n",
        "        save_progress_every_mb = 100  # Save every 100MB\n",
        "        bytes_since_last_save = 0\n",
        "\n",
        "        with tqdm(total=self.target_size_gb, unit='GB', desc=\"Processing C4\",\n",
        "                  initial=current_bytes/(1024**3)) as pbar:\n",
        "            try:\n",
        "                for item in dataset:\n",
        "                    # Skip if we've reached target\n",
        "                    if current_bytes >= target_bytes:\n",
        "                        print(\"\\nüéØ Target data size reached!\")\n",
        "                        break\n",
        "\n",
        "                    text = item['text']\n",
        "                    text_bytes = len(text.encode('utf-8'))\n",
        "\n",
        "                    # Check if adding this document would exceed target\n",
        "                    if current_bytes + text_bytes > target_bytes:\n",
        "                        print(f\"\\n‚ö†Ô∏è  Next document ({text_bytes/1024:.1f} KB) would exceed target\")\n",
        "                        print(\"Stopping here to stay within limit\")\n",
        "                        break\n",
        "\n",
        "                    # Tokenize\n",
        "                    tokens = self.tokenizer.encode(text)\n",
        "                    current_tokens.extend(tokens)\n",
        "                    current_tokens.append(self.tokenizer.eot_token)\n",
        "\n",
        "                    # Update progress\n",
        "                    current_bytes += text_bytes\n",
        "                    bytes_since_last_save += text_bytes\n",
        "\n",
        "                    pbar.update(text_bytes / (1024**3))\n",
        "                    pbar.set_postfix({\n",
        "                        'files': file_count,\n",
        "                        'tokens': f'{len(current_tokens):,}',\n",
        "                        'GB': f'{current_bytes/(1024**3):.2f}'\n",
        "                    })\n",
        "\n",
        "                    # Save progress periodically\n",
        "                    if bytes_since_last_save >= save_progress_every_mb * (1024**2):\n",
        "                        self.save_progress(current_bytes, file_count, current_tokens)\n",
        "                        bytes_since_last_save = 0\n",
        "\n",
        "                    # Save file when chunk is full\n",
        "                    if len(current_tokens) >= tokens_per_file:\n",
        "                        filename = f\"tokens_{file_count:04d}.npy\"\n",
        "\n",
        "                        # Save to local SSD first (FAST)\n",
        "                        token_array = np.array(current_tokens, dtype=np.uint16)\n",
        "                        local_file_path = self.local_temp_dir / filename\n",
        "                        np.save(local_file_path, token_array)\n",
        "\n",
        "                        # Copy to Drive with retry\n",
        "                        max_retries = 3\n",
        "                        for attempt in range(max_retries):\n",
        "                            try:\n",
        "                                shutil.copy(local_file_path, self.output_dir / filename)\n",
        "                                break\n",
        "                            except Exception as e:\n",
        "                                if attempt < max_retries - 1:\n",
        "                                    print(f\"\\n‚ö†Ô∏è  Copy failed (attempt {attempt+1}/{max_retries}): {e}\")\n",
        "                                    time.sleep(5)\n",
        "                                else:\n",
        "                                    print(f\"\\n‚ùå Failed to copy after {max_retries} attempts\")\n",
        "                                    raise\n",
        "\n",
        "                        # Clean up\n",
        "                        local_file_path.unlink()\n",
        "\n",
        "                        file_count += 1\n",
        "                        current_tokens = []\n",
        "\n",
        "                        # Save progress after each file\n",
        "                        self.save_progress(current_bytes, file_count, current_tokens)\n",
        "                        bytes_since_last_save = 0\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\n\\n‚ö†Ô∏è  Interrupted! Saving progress...\")\n",
        "                self.save_progress(current_bytes, file_count, current_tokens)\n",
        "                print(f\"‚úÖ Progress saved!\")\n",
        "                print(f\"üìä Processed: {current_bytes/(1024**3):.2f} GB\")\n",
        "                print(f\"üì¶ Complete files: {file_count}\")\n",
        "                print(f\"üî§ Partial tokens: {len(current_tokens):,}\")\n",
        "                print(\"\\nüí° Run this cell again to continue from here!\")\n",
        "                return list(self.output_dir.glob(\"tokens_*.npy\"))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚ùå Error occurred: {e}\")\n",
        "                print(\"üíæ Saving progress before exit...\")\n",
        "                self.save_progress(current_bytes, file_count, current_tokens)\n",
        "                raise\n",
        "\n",
        "        # Save any remaining tokens\n",
        "        if current_tokens:\n",
        "            filename = f\"tokens_{file_count:04d}.npy\"\n",
        "            print(f\"\\nüíæ Saving final chunk ({len(current_tokens):,} tokens)...\")\n",
        "            token_array = np.array(current_tokens, dtype=np.uint16)\n",
        "            local_file_path = self.local_temp_dir / filename\n",
        "            np.save(local_file_path, token_array)\n",
        "            shutil.copy(local_file_path, self.output_dir / filename)\n",
        "            local_file_path.unlink()\n",
        "            file_count += 1\n",
        "\n",
        "        # Clean up partial chunk file\n",
        "        if self.partial_chunk_file.exists():\n",
        "            self.partial_chunk_file.unlink()\n",
        "\n",
        "        # Final save\n",
        "        self.save_progress(current_bytes, file_count, [])\n",
        "\n",
        "        print(f\"\\n‚úÖ Preprocessing complete!\")\n",
        "        print(f\"üìä Total processed: {current_bytes/(1024**3):.2f} GB\")\n",
        "        print(f\"üì¶ Created {file_count} token files\")\n",
        "        print(f\"üíæ Location: {self.output_dir}\")\n",
        "\n",
        "        # Calculate total tokens\n",
        "        all_files = list(self.output_dir.glob(\"tokens_*.npy\"))\n",
        "        total_tokens = sum(len(np.load(f)) for f in all_files)\n",
        "        print(f\"üî¢ Total tokens: {total_tokens:,} ({total_tokens/1e9:.2f}B)\")\n",
        "        print(f\"üéØ Optimal model size: {total_tokens / 20 / 1e6:.0f}M parameters\")\n",
        "\n",
        "        return all_files\n",
        "\n",
        "# Run preprocessing\n",
        "print(\"‚ö†Ô∏è  This cell will download and tokenize C4 data\")\n",
        "print(\"‚ö†Ô∏è  It will take 2-6 hours and use Google Drive space\")\n",
        "run_preprocessing = input(\"\\nRun preprocessing now? (y/n): \")\n",
        "\n",
        "if run_preprocessing.lower() == 'y':\n",
        "    target_gb = int(input(\"How many GB of C4? (recommended: 50-100): \") or \"50\")\n",
        "    preprocessor = C4Preprocessor(Config.DRIVE_DATA_DIR, target_size_gb=target_gb)\n",
        "    token_files = preprocessor.preprocess_and_save()\n",
        "    print(f\"\\n‚úÖ Ready for training with {len(token_files)} token files!\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  Skipping preprocessing (assuming data already exists)\")"
      ],
      "metadata": {
        "id": "dLNCFQJHLD4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DOWNLOAD CELL 2: ONE-TIME DATASET PREPARATION\n",
        "# ============================================================================\n",
        "# ‚ö†Ô∏è  RUN THIS ONLY ONCE - Takes 3-5 hours\n",
        "# Downloads Cosmopedia, Alpaca, and optionally Python to Drive\n",
        "# After running, datasets are saved permanently - never run again!\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def download_cosmopedia(output_dir: Path, tokenizer, target_gb: float = 20.0):\n",
        "    \"\"\"\n",
        "    Download REAL Cosmopedia from HuggingFace.\n",
        "    Educational/textbook content for explainer-style answers.\n",
        "    \"\"\"\n",
        "    output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    print(f\"üì• Downloading Cosmopedia from HuggingFace...\")\n",
        "    print(f\"   Target: {target_gb} GB of educational content\")\n",
        "    print(f\"   This will take 1-3 hours...\")\n",
        "\n",
        "    try:\n",
        "        dataset = load_dataset(\n",
        "            \"HuggingFaceTB/cosmopedia\",\n",
        "            \"web_samples_v1\",\n",
        "            split=\"train\",\n",
        "            streaming=True,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(\"‚úÖ Loaded web_samples_v1 subset\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  web_samples_v1 failed: {e}\")\n",
        "        print(\"   Trying 'stories' subset...\")\n",
        "        dataset = load_dataset(\n",
        "            \"HuggingFaceTB/cosmopedia\",\n",
        "            \"stories\",\n",
        "            split=\"train\",\n",
        "            streaming=True,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(\"‚úÖ Loaded stories subset\")\n",
        "\n",
        "    all_tokens = []\n",
        "    target_bytes = int(target_gb * (1024**3))\n",
        "    current_bytes = 0\n",
        "    tokens_per_file = 5_000_000\n",
        "    file_count = 0\n",
        "\n",
        "    print(\"üîÑ Tokenizing Cosmopedia...\")\n",
        "\n",
        "    with tqdm(total=target_gb, unit='GB', desc=\"Progress\") as pbar:\n",
        "        try:\n",
        "            for item in dataset:\n",
        "                text = item.get('text', '').strip()\n",
        "\n",
        "                if not text:\n",
        "                    continue\n",
        "\n",
        "                text_bytes = len(text.encode('utf-8'))\n",
        "\n",
        "                if current_bytes + text_bytes > target_bytes:\n",
        "                    print(f\"\\nüéØ Reached target {target_gb} GB!\")\n",
        "                    break\n",
        "\n",
        "                tokens = tokenizer.encode(text)\n",
        "                all_tokens.extend(tokens)\n",
        "                all_tokens.append(tokenizer.eot_token)\n",
        "\n",
        "                current_bytes += text_bytes\n",
        "                pbar.update(text_bytes / (1024**3))\n",
        "\n",
        "                # Save chunk\n",
        "                if len(all_tokens) >= tokens_per_file:\n",
        "                    filename = output_dir / f\"cosmopedia_tokens_{file_count:04d}.npy\"\n",
        "                    np.save(filename, np.array(all_tokens[:tokens_per_file], dtype=np.uint16))\n",
        "                    all_tokens = all_tokens[tokens_per_file:]\n",
        "                    file_count += 1\n",
        "                    pbar.set_postfix({'files': file_count})\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n‚ö†Ô∏è  Interrupted! Saving progress...\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è  Error: {e}\")\n",
        "            print(\"Saving what we have...\")\n",
        "\n",
        "    # Save remaining\n",
        "    if all_tokens:\n",
        "        filename = output_dir / f\"cosmopedia_tokens_{file_count:04d}.npy\"\n",
        "        np.save(filename, np.array(all_tokens, dtype=np.uint16))\n",
        "        file_count += 1\n",
        "\n",
        "    print(f\"\\n‚úÖ Cosmopedia Complete!\")\n",
        "    print(f\"   üìä {current_bytes/(1024**3):.2f} GB raw text\")\n",
        "    print(f\"   üì¶ {file_count} token files\")\n",
        "    print(f\"   üíæ Saved to: {output_dir}\")\n",
        "\n",
        "    return list(output_dir.glob(\"cosmopedia_tokens_*.npy\"))\n",
        "\n",
        "\n",
        "def download_alpaca(output_dir: Path, tokenizer):\n",
        "    \"\"\"\n",
        "    Download and tokenize Alpaca instruction dataset.\n",
        "    Format: <|user|> instruction <|assistant|> response\n",
        "    \"\"\"\n",
        "    output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    print(f\"üì• Downloading Alpaca...\")\n",
        "\n",
        "    # Download if not exists\n",
        "    if not os.path.exists(\"alpaca_data.json\"):\n",
        "        os.system(\"wget -q https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\")\n",
        "\n",
        "    with open(\"alpaca_data.json\", 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"üîÑ Tokenizing {len(data)} Alpaca samples...\")\n",
        "\n",
        "    all_tokens = []\n",
        "    skipped = 0\n",
        "\n",
        "    for item in tqdm(data, desc=\"Processing\"):\n",
        "        instruction = item.get('instruction', '').strip()\n",
        "        input_text = item.get('input', '').strip()\n",
        "        output_text = item.get('output', '').strip()\n",
        "\n",
        "        if not instruction or not output_text:\n",
        "            skipped += 1\n",
        "            continue\n",
        "\n",
        "        # Strong formatting with anchors\n",
        "        if input_text:\n",
        "            text = f\"<|user|> {instruction}\\n{input_text}\\n<|assistant|> {output_text}\\n\"\n",
        "        else:\n",
        "            text = f\"<|user|> {instruction}\\n<|assistant|> {output_text}\\n\"\n",
        "\n",
        "        tokens = tokenizer.encode(text)\n",
        "        all_tokens.extend(tokens)\n",
        "        all_tokens.append(tokenizer.eot_token)\n",
        "\n",
        "    # Save in chunks\n",
        "    tokens_per_file = 5_000_000\n",
        "    file_count = 0\n",
        "\n",
        "    for i in range(0, len(all_tokens), tokens_per_file):\n",
        "        chunk = all_tokens[i:i + tokens_per_file]\n",
        "        filename = output_dir / f\"alpaca_tokens_{file_count:04d}.npy\"\n",
        "        np.save(filename, np.array(chunk, dtype=np.uint16))\n",
        "        file_count += 1\n",
        "\n",
        "    if skipped > 0:\n",
        "        print(f\"‚ö†Ô∏è  Skipped {skipped} empty samples\")\n",
        "\n",
        "    print(f\"‚úÖ Alpaca Complete!\")\n",
        "    print(f\"   üì¶ {file_count} token files\")\n",
        "    print(f\"   üìù ~{len(data) - skipped:,} instruction pairs\")\n",
        "    print(f\"   üíæ Saved to: {output_dir}\")\n",
        "\n",
        "    return list(output_dir.glob(\"alpaca_tokens_*.npy\"))\n",
        "\n",
        "\n",
        "def download_python(output_dir: Path, tokenizer, target_gb: float = 5.0):\n",
        "    \"\"\"\n",
        "    Download Python code dataset from HuggingFace.\n",
        "    Uses 'bigcode/the-stack-dedup' dataset filtered for Python.\n",
        "    \"\"\"\n",
        "    output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    print(f\"üì• Downloading Python code dataset...\")\n",
        "    print(f\"   Target: {target_gb} GB of Python code\")\n",
        "    print(f\"   This will take 30-60 minutes...\")\n",
        "\n",
        "    try:\n",
        "        # Option 1: Try bigcode/the-stack-dedup (most reliable)\n",
        "        print(\"   Trying bigcode/the-stack-dedup...\")\n",
        "        dataset = load_dataset(\n",
        "            \"bigcode/the-stack-dedup\",\n",
        "            data_dir=\"data/python\",\n",
        "            split=\"train\",\n",
        "            streaming=True,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(\"‚úÖ Loaded The Stack (Python)\")\n",
        "    except Exception as e1:\n",
        "        print(f\"   Failed: {e1}\")\n",
        "        try:\n",
        "            # Option 2: Try codeparrot/github-code-clean\n",
        "            print(\"   Trying codeparrot/github-code-clean...\")\n",
        "            dataset = load_dataset(\n",
        "                \"codeparrot/github-code-clean\",\n",
        "                languages=[\"Python\"],\n",
        "                split=\"train\",\n",
        "                streaming=True,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            print(\"‚úÖ Loaded GitHub Code Clean (Python)\")\n",
        "        except Exception as e2:\n",
        "            print(f\"   Failed: {e2}\")\n",
        "            try:\n",
        "                # Option 3: Try smaller dataset\n",
        "                print(\"   Trying sahil2801/CodeAlpaca-20k...\")\n",
        "                dataset = load_dataset(\n",
        "                    \"sahil2801/CodeAlpaca-20k\",\n",
        "                    split=\"train\",\n",
        "                    streaming=True,\n",
        "                    trust_remote_code=True\n",
        "                )\n",
        "                print(\"‚úÖ Loaded CodeAlpaca (smaller, but reliable)\")\n",
        "            except Exception as e3:\n",
        "                print(f\"‚ö†Ô∏è  All Python datasets failed:\")\n",
        "                print(f\"      Option 1: {e1}\")\n",
        "                print(f\"      Option 2: {e2}\")\n",
        "                print(f\"      Option 3: {e3}\")\n",
        "                print(\"   Skipping Python dataset...\")\n",
        "                print(\"   üí° You can train without it - just set python: 0.0 in probs\")\n",
        "                return []\n",
        "\n",
        "    all_tokens = []\n",
        "    target_bytes = int(target_gb * (1024**3))\n",
        "    current_bytes = 0\n",
        "    tokens_per_file = 10_000_000\n",
        "    file_count = 0\n",
        "\n",
        "    print(\"üîÑ Tokenizing Python code...\")\n",
        "\n",
        "    with tqdm(total=target_gb, unit='GB', desc=\"Progress\") as pbar:\n",
        "        try:\n",
        "            for item in dataset:\n",
        "                # Try different field names (datasets vary)\n",
        "                code = None\n",
        "                for field in ['content', 'code', 'text', 'output']:\n",
        "                    if field in item and item[field]:\n",
        "                        code = item[field].strip()\n",
        "                        break\n",
        "\n",
        "                if not code or len(code) < 100:  # Skip tiny snippets\n",
        "                    continue\n",
        "\n",
        "                code_bytes = len(code.encode('utf-8'))\n",
        "\n",
        "                if current_bytes + code_bytes > target_bytes:\n",
        "                    print(f\"\\nüéØ Reached target {target_gb} GB!\")\n",
        "                    break\n",
        "\n",
        "                tokens = tokenizer.encode(code)\n",
        "                all_tokens.extend(tokens)\n",
        "                all_tokens.append(tokenizer.eot_token)\n",
        "\n",
        "                current_bytes += code_bytes\n",
        "                pbar.update(code_bytes / (1024**3))\n",
        "\n",
        "                # Save chunk\n",
        "                if len(all_tokens) >= tokens_per_file:\n",
        "                    filename = output_dir / f\"python_tokens_{file_count:04d}.npy\"\n",
        "                    np.save(filename, np.array(all_tokens[:tokens_per_file], dtype=np.uint16))\n",
        "                    all_tokens = all_tokens[tokens_per_file:]\n",
        "                    file_count += 1\n",
        "                    pbar.set_postfix({'files': file_count})\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n‚ö†Ô∏è  Interrupted! Saving progress...\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è  Error during processing: {e}\")\n",
        "            print(\"Saving what we have...\")\n",
        "\n",
        "    # Save remaining\n",
        "    if all_tokens:\n",
        "        filename = output_dir / f\"python_tokens_{file_count:04d}.npy\"\n",
        "        np.save(filename, np.array(all_tokens, dtype=np.uint16))\n",
        "        file_count += 1\n",
        "\n",
        "    if file_count == 0:\n",
        "        print(f\"\\n‚ö†Ô∏è  No Python data collected!\")\n",
        "        print(\"   Training will continue without Python dataset\")\n",
        "        return []\n",
        "\n",
        "    print(f\"\\n‚úÖ Python Complete!\")\n",
        "    print(f\"   üìä {current_bytes/(1024**3):.2f} GB code\")\n",
        "    print(f\"   üì¶ {file_count} token files\")\n",
        "    print(f\"   üíæ Saved to: {output_dir}\")\n",
        "\n",
        "    return list(output_dir.glob(\"python_tokens_*.npy\"))\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ONE-TIME DATASET PREPARATION\")\n",
        "print(\"=\"*70)\n",
        "print(\"This cell downloads and tokenizes:\")\n",
        "print(\"  1. Cosmopedia (educational content) - ~20GB, 1-3 hours\")\n",
        "print(\"  2. Alpaca (instruction following) - ~100MB, 5 minutes\")\n",
        "print(\"  3. Python (code, optional) - ~5GB, 30-60 minutes\")\n",
        "print(\"\")\n",
        "print(\"‚ö†Ô∏è  WARNING: This will use ~50GB of Google Drive space\")\n",
        "print(\"‚ö†Ô∏è  WARNING: Only run this ONCE - data persists in Drive\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if already exists\n",
        "base_dir = Path(Config.DRIVE_DATA_DIR)\n",
        "cosmopedia_exists = len(list((base_dir / \"cosmopedia\").glob(\"*.npy\"))) > 0\n",
        "alpaca_exists = len(list((base_dir / \"alpaca\").glob(\"*.npy\"))) > 0\n",
        "python_exists = len(list((base_dir / \"python\").glob(\"*.npy\"))) > 0\n",
        "\n",
        "print(f\"\\nüìä Current Status:\")\n",
        "print(f\"   Cosmopedia: {'‚úÖ Already exists' if cosmopedia_exists else '‚ùå Not found'}\")\n",
        "print(f\"   Alpaca: {'‚úÖ Already exists' if alpaca_exists else '‚ùå Not found'}\")\n",
        "print(f\"   Python: {'‚úÖ Already exists' if python_exists else '‚ùå Not found (optional)'}\")\n",
        "\n",
        "if cosmopedia_exists and alpaca_exists:\n",
        "    print(\"\\n‚úÖ All required datasets already exist!\")\n",
        "    print(\"   You can skip this cell and go directly to Cell 5B\")\n",
        "    should_run = input(\"\\nRe-download anyway? (yes/no): \").strip().lower()\n",
        "    if should_run != 'yes':\n",
        "        print(\"Skipping download.\")\n",
        "        import sys\n",
        "        sys.exit(0)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "run_confirmation = input(\"Start downloading? Type 'yes' to continue: \").strip().lower()\n",
        "\n",
        "if run_confirmation != 'yes':\n",
        "    print(\"‚ùå Download cancelled\")\n",
        "else:\n",
        "    print(\"\\nüöÄ Starting download and tokenization...\\n\")\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # 1. Cosmopedia\n",
        "    if not cosmopedia_exists:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"DOWNLOADING COSMOPEDIA\")\n",
        "        print(\"=\"*70)\n",
        "        cosmopedia_files = download_cosmopedia(\n",
        "            base_dir / \"cosmopedia\",\n",
        "            tokenizer,\n",
        "            target_gb=20.0\n",
        "        )\n",
        "    else:\n",
        "        print(\"\\n‚è≠Ô∏è  Skipping Cosmopedia (already exists)\")\n",
        "\n",
        "    # 2. Alpaca\n",
        "    if not alpaca_exists:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"DOWNLOADING ALPACA\")\n",
        "        print(\"=\"*70)\n",
        "        alpaca_files = download_alpaca(\n",
        "            base_dir / \"alpaca\",\n",
        "            tokenizer\n",
        "        )\n",
        "    else:\n",
        "        print(\"\\n‚è≠Ô∏è  Skipping Alpaca (already exists)\")\n",
        "\n",
        "    # 3. Python (optional)\n",
        "    if not python_exists:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"DOWNLOADING PYTHON (OPTIONAL)\")\n",
        "        print(\"=\"*70)\n",
        "        want_python = input(\"Download Python dataset? (yes/no): \").strip().lower()\n",
        "        if want_python == 'yes':\n",
        "            python_files = download_python(\n",
        "                base_dir / \"python\",\n",
        "                tokenizer,\n",
        "                target_gb=5.0\n",
        "            )\n",
        "        else:\n",
        "            print(\"‚è≠Ô∏è  Skipping Python dataset\")\n",
        "    else:\n",
        "        print(\"\\n‚è≠Ô∏è  Skipping Python (already exists)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ ALL DOWNLOADS COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Next steps:\")\n",
        "    print(\"  1. Run Cell 5B to create the mixed dataloader\")\n",
        "    print(\"  2. Run Cell 9 to continue training\")\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "id": "zreqwxKsK-hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DOWNLOAD CELL 3: PYTHON DATASET DOWNLOAD (Instruction-Aligned, Logic-Focused)\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def download_python_dataset(output_dir: Path, tokenizer, target_gb: float = 5.0):\n",
        "    \"\"\"\n",
        "    Download Python instruction-style dataset.\n",
        "    Python is used to teach LOGICAL ANSWERING, not raw code completion.\n",
        "    \"\"\"\n",
        "    output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"DOWNLOADING PYTHON DATASET (LOGIC-ALIGNED)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Target: {target_gb} GB of instruction-style Python\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    dataset = None\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # OPTION 1: CodeAlpaca (BEST for your goal)\n",
        "    # ------------------------------------------------------------\n",
        "    print(\"üì• Option 1: Trying 'sahil2801/CodeAlpaca-20k'...\")\n",
        "    try:\n",
        "        dataset = load_dataset(\n",
        "            \"sahil2801/CodeAlpaca-20k\",\n",
        "            split=\"train\",\n",
        "            streaming=True,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(\"‚úÖ Loaded CodeAlpaca\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed: {str(e)[:100]}...\\n\")\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # OPTION 2: Python instruction fallback\n",
        "    # ------------------------------------------------------------\n",
        "    if dataset is None:\n",
        "        print(\"üì• Option 2: Trying 'iamtarun/python_code_instructions_18k_alpaca'...\")\n",
        "        try:\n",
        "            dataset = load_dataset(\n",
        "                \"iamtarun/python_code_instructions_18k_alpaca\",\n",
        "                split=\"train\",\n",
        "                streaming=True,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            print(\"‚úÖ Loaded Python Instructions\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed: {str(e)[:100]}...\\n\")\n",
        "\n",
        "    if dataset is None:\n",
        "        print(\"‚ùå No suitable Python instruction dataset found.\")\n",
        "        print(\"Training will continue WITHOUT Python.\")\n",
        "        return []\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # TOKENIZATION (CRITICAL FIX: instruction alignment)\n",
        "    # ------------------------------------------------------------\n",
        "    all_tokens = []\n",
        "    target_bytes = int(target_gb * (1024**3))\n",
        "    current_bytes = 0\n",
        "    tokens_per_file = 10_000_000\n",
        "    file_count = 0\n",
        "\n",
        "    print(\"üîÑ Tokenizing Python instructions...\")\n",
        "    print(\"Python will reinforce reasoning + response discipline\\n\")\n",
        "\n",
        "    with tqdm(total=target_gb, unit='GB', desc=\"Progress\") as pbar:\n",
        "        try:\n",
        "            for item in dataset:\n",
        "                instruction = str(item.get(\"instruction\", \"\")).strip()\n",
        "                output = str(item.get(\"output\", \"\")).strip()\n",
        "\n",
        "                if not instruction or not output:\n",
        "                    continue\n",
        "\n",
        "                # STRONG alignment with Alpaca / Cosmopedia\n",
        "                text = f\"<|user|> {instruction}\\n<|assistant|> {output}\\n\"\n",
        "                text_bytes = len(text.encode(\"utf-8\"))\n",
        "\n",
        "                if current_bytes + text_bytes > target_bytes:\n",
        "                    print(\"\\nüéØ Target reached\")\n",
        "                    break\n",
        "\n",
        "                tokens = tokenizer.encode(text)\n",
        "                all_tokens.extend(tokens)\n",
        "                all_tokens.append(tokenizer.eot_token)\n",
        "\n",
        "                current_bytes += text_bytes\n",
        "                pbar.update(text_bytes / (1024**3))\n",
        "\n",
        "                if len(all_tokens) >= tokens_per_file:\n",
        "                    filename = output_dir / f\"python_tokens_{file_count:04d}.npy\"\n",
        "                    np.save(filename, np.array(all_tokens[:tokens_per_file], dtype=np.uint16))\n",
        "                    all_tokens = all_tokens[tokens_per_file:]\n",
        "                    file_count += 1\n",
        "                    pbar.set_postfix({\n",
        "                        \"files\": file_count,\n",
        "                        \"GB\": f\"{current_bytes/(1024**3):.2f}\"\n",
        "                    })\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n‚ö†Ô∏è Interrupted, saving progress...\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è Error: {e}\")\n",
        "            print(\"Saving what we have...\")\n",
        "\n",
        "    # Save remainder\n",
        "    if all_tokens:\n",
        "        filename = output_dir / f\"python_tokens_{file_count:04d}.npy\"\n",
        "        np.save(filename, np.array(all_tokens, dtype=np.uint16))\n",
        "        file_count += 1\n",
        "\n",
        "    if file_count == 0:\n",
        "        print(\"‚ùå No Python data collected\")\n",
        "        return []\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ PYTHON DATASET READY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"üì¶ Files: {file_count}\")\n",
        "    print(f\"üìä Raw text: {current_bytes/(1024**3):.2f} GB\")\n",
        "    print(f\"üíæ Location: {output_dir}\")\n",
        "    print(\"üéØ Purpose: Logical answering + structured responses\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return list(output_dir.glob(\"python_tokens_*.npy\"))\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# RUN\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nüêç PREPARING PYTHON DATASET (LOGIC MODE)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "python_dir = Path(Config.DRIVE_DATA_DIR) / \"python\"\n",
        "\n",
        "python_files = download_python_dataset(\n",
        "    python_dir,\n",
        "    tokenizer,\n",
        "    target_gb=5.0\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Python files ready: {len(python_files)}\")\n",
        "print(\"Next step: plug into MixedDatasetLoader\")"
      ],
      "metadata": {
        "id": "S435xy_ALF_z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}